<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Matrix Calculus Cheatsheet</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Matrix Calculus Cheatsheet</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>To train machine learning models, we use the standard SGD update rule (or some derived variant) defined as</p>
<p><span class="math display">\[\theta^{\text {new }}=\theta^{\text {old }}-\alpha \nabla_\theta J(\theta)\]</span></p>
<p>However, of course, this requires us to compute <span class="math inline">\(\nabla_\theta J(\theta)\)</span>, the gradient of the loss function with respect to our model weights/parameters. That is, for each parameter, we will need to calculate</p>
<p><span class="math display">\[\theta_j^{\text {new }}=\theta_j^{\text {old }}-\alpha \frac{\partial J(\theta)}{\partial \theta_j^{\text {old }}}.\]</span></p>
<p>For the rest of the class, we will discuss how to calculate <span class="math inline">\(\nabla_\theta J(\theta)\)</span> in two different ways:</p>
<ol type="1">
<li>By hand</li>
<li>Algorithmically: the backpropagation algorithm</li>
</ol>
<section id="chain-rule" class="level3">
<h3 class="anchored" data-anchor-id="chain-rule">Chain rule</h3>
<p>The backbone of backprop is the chain rule which allows us to compute derivatives on compositions of functions.</p>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Scalar-valued function) </strong></span><span class="math display">\[z=3 y\]</span> <span class="math display">\[y=x^2\]</span> <span class="math display">\[\frac{d z}{d x}=\frac{d z}{d y} \frac{d y}{d x}=(3)(2 x)=6 x\]</span></p>
</div>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Vector-valued function) </strong></span><span class="math display">\[\boldsymbol{h}=f(\boldsymbol{z})\]</span> <span class="math display">\[\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}\]</span> <span class="math display">\[\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}}=\ldots\]</span></p>
</div>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (Derivation of neural network gradient) </strong></span>Say we have a one layer neural net where we take from input vector <span class="math inline">\(\boldsymbol{x}\)</span> and want to produce a singular output neuron called the score. Diagram:</p>
<p align="center">
<img alt="picture 1" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/16639f74e6e91122675e5be41335e78db68f8733c75a9953df3039fc3b8c282c.png" width="300">
</p>
<p>key realization: let’s break up the equations into simpler pieces use placeholder variables:</p>
<p><span class="math display">\[\boldsymbol{h}=f(\boldsymbol{z})\]</span> where <span class="math display">\[\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}\]</span></p>
<p>Now say we want <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{b}}\)</span> all we got to do is apply the chain rule on</p>
<p align="center">
<img alt="picture 2" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/bebf557b28605825afcc49133aba08ca5de7774f6312d2bdd11458aee9ac92fb.png" width="20%">
</p>
<p>for which we get</p>
<p><span class="math display">\[
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} = \boldsymbol{u}^T \operatorname{diag}\left(f^{\prime}(\boldsymbol{z})\right) \boldsymbol{I} = \boldsymbol{u}^T \odot f^{\prime}(\boldsymbol{z}).
\]</span></p>
<p>Now try calculating <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span>. Answer:</p>
<p><span class="math display">\[\frac{\partial s}{\partial \boldsymbol{W}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}\]</span></p>
<p>Aside: if we need to calculate both <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span> and <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{b}}\)</span>, then we are redudant:</p>
<p align="center">
<img alt="picture 3" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/408623306770ca64cd5cd902cd7e07640e5647ae5e592deb12b6dc992a7f40f0.png" width="20%">
</p>
<p>to resolve this redundancy, we can define a variable <span class="math inline">\(\boldsymbol{\delta}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}\)</span>. This memoization concept is key in backpropogation.</p>
</div>
</section>
<section id="function-shapes" class="level3">
<h3 class="anchored" data-anchor-id="function-shapes">Function shapes</h3>
<p>See <a href="https://www.cs.princeton.edu/courses/archive/fall20/cos302/notes/cos302_f20_lecture12_vectordiff.pdf">here</a>.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em> (<strong>Derivative shape </strong>). </span>Let <span class="math display">\[f: \mathbb{R}^{\text {(input shape) }} \rightarrow \mathbb{R}^{\text {(output shape) }}.\]</span> Then its derivative will be a function of shape <span class="math display">\[\nabla f: \mathbb{R}^{(\text {input shape) }} \rightarrow \mathbb{R}^{(\text {output shape }) \times(\text { input shape) }}.\]</span></p>
<p>Each extra added dimension on the output corresponds to taking partial derivatives with respect to the all of the input dimension (think of the gradient, for example). Meaning, for each output, we need to take the derivative with respect to all inputs seperately.</p>
</div>
</div>
</div>
</div>
<ul>
<li>Function of a scalar, returning a scalar: <span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span>
<ul>
<li>Example: <span class="math inline">\(f(x)=a x+b\)</span></li>
<li>Derivative shape: <span class="math inline">\(\frac{d f}{d x}: \mathbb{R} \rightarrow \mathbb{R}\)</span> (derivative)</li>
</ul></li>
<li>Function of a scalar, returning a vector: <span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}^n\)</span>
<ul>
<li>Example: <span class="math inline">\(f(x)=x v\)</span></li>
<li>Derivative shape: <span class="math inline">\(\frac{d f}{d x}: \mathbb{R} \rightarrow \mathbb{R}^n\)</span> (vector-valued derivative)</li>
</ul></li>
<li>Function of a vector, returning a scalar: <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}\)</span>
<ul>
<li>Example: <span class="math inline">\(f(\boldsymbol{x})=\boldsymbol{v}^{\top} \boldsymbol{x}\)</span></li>
<li>Derivative shape: <span class="math inline">\(\nabla f(\boldsymbol{x}): \mathbb{R}^n \rightarrow \mathbb{R}^{1 \times n}\)</span> (gradient)</li>
</ul></li>
<li>Function of a vector, returning a vector: <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>
<ul>
<li>Example: <span class="math inline">\(f(x)=M x\)</span></li>
<li>Derivative shape:<span class="math inline">\(\mathrm{J}(f(\boldsymbol{x})): \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}\)</span> (jacobian)</li>
</ul></li>
<li>Function of a vector, returning a matrix: <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}\)</span>
<ul>
<li>Example: <span class="math inline">\(\boldsymbol{F}(\boldsymbol{x})=\boldsymbol{x} \boldsymbol{x}^{\top}\)</span></li>
<li>Derivative shape: <span class="math inline">\(\nabla \boldsymbol{F}(\boldsymbol{x}): \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n \times n}\)</span> (generalized Jacobian)</li>
</ul></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Many more examples
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>see cos 302 slides.</p>
</div>
</div>
</div>
<section id="neural-network-output-shape" class="level4">
<h4 class="anchored" data-anchor-id="neural-network-output-shape">Neural network output shape</h4>
<p>You’ll notice a flaw in our machinery. The above shapes are true in pure mathematics. But in neural network land, we want to do the following SGD update:</p>
<p><span class="math display">\[\theta^{\text {new }}=\theta^{\text {old }}-\alpha \nabla_\theta J(\theta)\]</span></p>
<p>However, if our <span class="math inline">\(\nabla_\theta J(\theta)\)</span> shape is different from our old parameter shape <span class="math inline">\(\theta^{\text {old }}\)</span>, we can’t possibly do this update–the shapes don’t match. So we define our new shape convention as <strong>the shape of the gradient is the shape of the parameters <span class="math inline">\(\theta\)</span></strong>. Example: <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span> will be of shape <span class="math inline">\(n \times m\)</span>: <span class="math display">\[\left[\begin{array}{ccc}\frac{\partial s}{\partial W_{11}} &amp; \cdots &amp; \frac{\partial s}{\partial W_{1 m}} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial s}{\partial W_{n 1}} &amp; \cdots &amp; \frac{\partial s}{\partial W_{n m}}\end{array}\right].\]</span></p>
<p>This leads us into reshaping and tranpose: two tools we’ll need to get the SGD update to work shape-wise.</p>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 4 </strong></span>Using our neural network example from above, we want to compute <span class="math inline">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span>. We know that the shape should be the shape of <span class="math inline">\(\boldsymbol{W}\)</span> which is <span class="math inline">\(n \times m\)</span>. So calculate our derivative as normal:</p>
<p><span class="math display">\[\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\delta}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\boldsymbol{u}^T \circ f^{\prime}(\boldsymbol{z})\)</span> (which is of shape <span class="math inline">\(1 \times n\)</span>). So our answer is:</p>
<p><span class="math display">\[\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}
\boldsymbol{x}
\]</span></p>
<p>But these dimensions don’t give us our desired <span class="math inline">\(n \times m\)</span> shape. So let’s use the tranpose to get an <em>outer product</em> (a hacky trick):</p>
<p><span class="math display">\[\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^T \boldsymbol{x}^T\]</span></p>
<p>Breaking down the shapes:</p>
<p align="center">
<img alt="picture 4" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/954b54e6b79129e8e88aa7782dfdd37e7d846106d52db7574654955859f5acda.png" width="55%">
</p>
<p>This shape convention idea is all tied to neural networks because each derivative component in the outputted derivative is with respect to one edge between the current layer and the next (we need a component for each edge):</p>
<p align="center">
<img alt="picture 5" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/ca2563a333664bbbc61ccbd812e502a1e51288b196c5adecef5aaf1322f01ffe.png" width="30%">
</p>
</div>
</section>
</section>
<section id="backpropogation" class="level3">
<h3 class="anchored" data-anchor-id="backpropogation">Backpropogation</h3>
<p>How can we do everything we discussed above (matrix calculus), but in an <strong>efficient</strong> manner for many layer deep neural networks with millions of parameters? Backpropogation is an algorithm that accomplishes these two feats.</p>
<p>The main ideas behind backprop include memoization of upstream gradients and computation graphs.</p>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Computation graph) </strong></span>A directed acyclic graph whose root node represents the final mathematical expression and each node represents intermediate subexpressions. “forward propogation”.</p>
</div>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 5 (Forward propogation) </strong></span>For:</p>
<p align="center">
<img alt="picture 6" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/7c5bbabd9d34ba61728fe93feee25749e577726a52e6eecb06fa2280e47315ca.png" width="18%">
</p>
<p>we have:</p>
<p align="center">
<img alt="picture 8" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/f0107e3ea11aab3b0295990efc3f7c2a6c1aa3bf1d339dc1298f6a135523403c.png" width="400">
</p>
<p>Two things:</p>
<ul>
<li>Nodes represent the operations (besides input/output nodes)</li>
<li>Edges pass along result of the operation</li>
</ul>
</div>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 6 (Back propogation) </strong></span>Now calculate the gradients with respect to each node/edge needed of the final expression (<span class="math inline">\(s\)</span>) with respect to the parameter you are updating (go backwards along the edges):</p>
<p align="center">
<img alt="picture 9" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/ee128e4f9a81683355d5c2af7f53354500d23e7e62f845c4881a650de1a43398.png" width="400">
</p>
<p>Each node has a <strong>local gradient</strong>: The gradient of its output with respect to its input (i.e., the edge going into the operation/node and the edge going out of the operation/node). Each node receives (from the right hand side since we are going backwards) an “<strong>upstream gradient</strong>”. The goal then is to calculate and pass along the correct <strong>downstream gradient</strong> where <code>[downstream gradient] = [upstream gradient] x [local gradient]</code>.</p>
<p align="center">
<img alt="picture 11" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/e07fb44a623864685bea0fcb7276c826d841f2e38a6c07826b3d64c7d5f6298a.png" width="400">
</p>
<p>Then use the <strong>chain rule</strong> to piece together each downstream gradient until you arrive at the desired gradient:</p>
<p><span class="math display">\[\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}\]</span></p>
<p><em>Remark</em>: notice that each downstream gradient is a derivative of the final expression <span class="math inline">\(s\)</span> (i.e., the global gradient). However, each local gradient which comprises the downstream global gradient is a derivative with respect to the local function.</p>
</div>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 7 (Back propogation with multiple inputs) </strong></span>Of course, neural network layers have many inputs for each hidden layer node. A simplified version of the neural network hidder layer evaluation expression is:</p>
<p><span class="math display">\[z=\boldsymbol{W} x.\]</span></p>
<p>The concept in the previous example generalizes for multiple inputs; just calculate the downstream gradient for each respective input! Then follow along the path of the desired downstream gradient.</p>
<p align="center">
<img alt="picture 12" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/973fe8a56896a6b40672e7188baf7489075cd581d643ec6daca93f33469df2d1.png" width="400">
</p>
</div>
<p>Let’s do another example of a simple function to solidy our understanding.</p>
<div id="exm-default" class="theorem example">
<p><span class="theorem-title"><strong>Example 8 (Forward pass computation graph construction) </strong></span>Take the following function <span class="math inline">\(f: \mathbb{R}^{3} \rightarrow \mathbb{R}\)</span>.</p>
<p><span class="math display">\[f(x, y, z)=(x+y) \max (y, z)\]</span></p>
<p>We want to construct a computation graph for the forward pass.</p>
<ol type="1">
<li>Break down each “operation” into its own function and define that as a node.</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
&amp; a=x+y \\
&amp; b=\max (y, z) \\
&amp; f=a b
\end{aligned}
\]</span></p>
<ol start="2" type="1">
<li>Formulate the graph where the edges are the inputs and the nodes are the sub-operation functions.</li>
</ol>
<p align="center">
<img alt="picture 13" src="https://cdn.jsdelivr.net/gh/minimatest/vscode-images@main/images/fcb95fa2716d5277612e376d8b5b31f1fde509e7385ef8f590086bc3b1ed7379.png" width="400">
</p>
</div>
</section>




<div id="quarto-appendix" class="default"><section id="appendix-a-useful-identities" class="level2 appendix"><h2 class="quarto-appendix-heading">Appendix A: Useful Identities</h2><div class="quarto-appendix-contents">

<blockquote class="blockquote">
<p>Here we enumerate a bunch of helpful derivatives, simplifications, identities etc. in this section.</p>
</blockquote>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Gradient) </strong></span>yo</p>
</div>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Jacobian) </strong></span>yo</p>
</div>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Various jacobian derivatives) </strong></span><span class="math display">\[\frac{\partial}{\partial \boldsymbol{x}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{W}\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{I}\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \boldsymbol{u}}\left(\boldsymbol{u}^T \boldsymbol{h}\right)=\boldsymbol{h}^T\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \boldsymbol{z}}(f(\boldsymbol{z}))=\operatorname{diag}\left(f^{\prime}(\boldsymbol{z})\right)\]</span></p>
</div>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Sigmoid derivative) </strong></span><span class="math display">\[\sigma(z) = \frac{1}{1+e^{-z}}\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_j} \sigma(z)=\sigma(z)[1-\sigma(z)]\]</span></p>
</div>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (Log identities) </strong></span>Attaching a log before taking the derivative often helps us simplify the calculation since we can use the following identities:</p>
<p><span class="math display">\[\log \left(\frac{x}{y}\right)=\log (x)-\log(y)\]</span></p>
<p>…</p>
</div>
<div id="def-default" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Log-exponential cancelation) </strong></span>You’ll often come across the softmax normalization function:</p>
<p><span class="math display">\[s\left(x_i\right)=\frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\]</span></p>
<p>which normalizes the distribution such that it sums to 1. It involves an <code>exp</code>. If we tack on a log before we take this gradient, these will cancel out. For example:</p>
<p><span class="math display">\[
\frac{\partial}{\partial v_{c}} \log \exp \left(u_0^{\top} v_c\right)=\frac{\partial}{\partial v_c} u_0^{\top} v_c
\]</span></p>
</div>
</div></section><section id="appendix-b-more-matrix-calculus-examples" class="level2 appendix"><h2 class="quarto-appendix-heading">Appendix B: More matrix calculus examples</h2><div class="quarto-appendix-contents">

<p>Taking gradients of loss functions with respect to matrix weight parameters is a key concept used in many machine learning classes–so you’ll want to nail this concept down. To this end, we will provide many examples of taking gradients of loss functions with the hope of picking up useful patterns and identities (i.e., gradients of log).</p>
<p><em>To be updated eventually</em>.</p>
<p>Example sources:</p>
<ul>
<li>CS 229</li>
<li>CS 224N/CS231N</li>
<li>CS 221 exams</li>
</ul>
</div></section><section id="appendix-c-useful-resources" class="level2 appendix"><h2 class="quarto-appendix-heading">Appendix C: Useful resources</h2><div class="quarto-appendix-contents">

<ul>
<li>COS 302 slides</li>
<li>Matrix cookbook</li>
<li>https://explained.ai/matrix-calculus/</li>
</ul>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>